%!TeX root = ./../MusterAbschlussarbeit.tex

%##########################################################
% Inhalt
%##########################################################

\clearpage
\chapter{Auswertung und Ausblick}

\section{Bewertung der Ergebnisse}

Die Experimente aus dem vorherigen Kapiteln zeigen deutlich, dass der Trainingsfortschritt des Agenten vorhanden war.
Das trainierte Modell erhielt deutlich mehr Belohnungen als die untrainierten oder mit speziellen Lernumgebungen trainierten Agenten. Leider lässt sich dieser Fortschritt nur bedingt auf die Metrik der durchschnittlich erreichten Punkte anwenden. Wünschenswert wäre ein Modell gewesen, welches im Durchschnitt positive Spielergebnisse sammelt. Dem Agenten gelang es nicht kontinuierlich nach einer Strategie zu spielen, welche viele Punkte nach sich zieht. So konnte er fast nie eine Farbe komplett ausfüllen, was viele Punkte bringen würde. Dies liegt vorallem daran, dass er dieses Ziel von dem Agenten nur sehr selten erreicht wurde und somit auch nicht erlernt werden konnte.\\
Das Ergebnis auf einem anderen Spielfeld war für den trainierten Agenten überraschend schlecht. Dies spricht dafür, dass das Training auf einem einzelnen Spielfeld nicht förderlich ist für ein Modell, das auf verschiedene Situationen reagieren soll. \\ Während des Verlaufs dieser Arbeit, musste ich feststellen, dass das Problem, ein wenig kompliziertes Spiel wie  'Noch mal!' zu erlernen, für einen RL Agenten deutlich komplexer ist, als erwartet. Der Agent musste eine Vielzahl von Daten verarbeiten und diese semantisch zuordnen. Dies führte zu einer immensen Dauer des Trainingsprozesses. Der Agent, welcher 25 Mio. Trainingsschritte durchlaufen hatte, brauchte für diesen Prozess etwa 30 Stunden. Während dieser Zeit konnte das Modell nur minimale Verbesserungen aufweisen. Weiterhin kommt hinzu, dass das modelierte Spiel ein Glücksspiel ist. Es kann vorkommen, dass der Spieler Pech im Würfeln hat und so trotz perfekter Spielweise ein schlechtes Spielergebnis erziehlt. Deshalb ist es auch nicht möglich ein Modell zu erzeugen, was immer ein gutes Ergebnis liefert. Allerdings wäre mit weiterer Rechenzeit und vermehrter Nutzung von verschieden Trainingsarten noch eine weitere Verbesserung des Modells möglich.

\section{Schritte zur Verbesserung des Agenten}

Mit mehr Rechenleistung und Trainingszeit könnte der Agent durchaus noch zu besseren Ergebnissen gelangen. Ein Flaschenhals war unter anderem, dass jede Trainingsiteration von 1 Mio. Schritten ungefähr 1,5 Stunden dauerte. Falls es zu Fehlern während des Trainings kam, musste die aufgewendete Zeit erneut investiert werden. Weiterhin konnte sich der Agent, wenn auch nur langsam, stetig verbessern. Mit einer deutlich erhöhten Trainingsdauer, kann das Modell deutlich verbessert werden.\\
Ein kompletter Verzicht der Visualisierung während des Trainings kann zu einer Verbesserung der Trainingsdauer führen. Es würden mehr Ressourcen zur Berechnung des neuronalen Netzes  zur Verfügung stehen und damit den Prozess verkürzen. Weiterhin kam es insbesondere während langen Trainingsepisoden zu Crashes, welche durch Unity hervorgerufen wurden. Dies könnte durch eine performantere Lernumgebung substituiert werden, um einen robusteren Ablauf zu erschaffen. \\
Ein weiteres Problem des trainierten Modells war die Überanpassung. Um dies zu umgehen und Modelle zu trainieren, welche auf verschiedene Zustände des Spielfeldes optimal reagieren können, wäre es hilfreich das Spielfeld für jede Spielrunde zufällig generisch zu erzeugen. Dies würde dazu führen, dass der Agent nicht ein einzelnes Spielfeld , sondern das Spielen des Spiels erlernt. 
Generisch erzeugte Trainingsumgebungen würden die Trainingsdauer erhöhen, dafür aber in ein robusteres Modell resultieren. \\
Anpassen der Hyperparameter des Agenten können zu einer weiteren Optimierung des Trainingsprozesses führen. Leider ist das Anpassen der Hyperparameter nicht intuitiv nachvollziehbar, weshalb Erfahrung im Umgang mit RL erforderlich ist. Durch geschicktes Verändern dieser Parameter lassen sich lokale Maxima überspringen oder die Exploration des Agenten beschleunigen. \\
Eine weitere Verbesserung des Trainingsprozessen könnte auch eine ausgedehnte Selektion nach Vorbild der Evolutionären Algorithmen liefern. Dies hätte einen höheren Aufwand der Trainings zur Folge, könnte aber dazu führen, dass sich gute Modelle durchsetzen die schneller eine optimale Spielstrategie etablieren. 