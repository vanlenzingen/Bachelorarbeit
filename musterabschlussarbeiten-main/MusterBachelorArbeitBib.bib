
@book{zai_einstieg_2020,
	address = {München},
	title = {Einstieg in {Deep} {Reinforcement} {Learning}: {KI}-{Agenten} mit {Python} und {PyTorch} programmieren},
	isbn = {978-3-446-45900-7},
	shorttitle = {Einstieg in {Deep} {Reinforcement} {Learning}},
	language = {en},
	publisher = {Hanser},
	author = {Zai, Alex and Brown, Brandon},
	year = {2020},
	file = {Zai and Brown - 2020 - Einstieg in Deep Reinforcement Learning KI-Agente.pdf:/home/tony/Zotero/storage/HXZRLXVX/Zai and Brown - 2020 - Einstieg in Deep Reinforcement Learning KI-Agente.pdf:application/pdf},
}

@book{sutton_reinforcement_2014,
	address = {Cambridge, Massachusetts},
	edition = {Nachdruck},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-19398-6},
	shorttitle = {Reinforcement learning},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew},
	year = {2014},
	file = {Sutton and Barto - 2014 - Reinforcement learning an introduction.pdf:/home/tony/Zotero/storage/I3L62CDK/Sutton and Barto - 2014 - Reinforcement learning an introduction.pdf:application/pdf},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the beneﬁts of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	language = {en},
	urldate = {2024-01-26},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:/home/tony/Zotero/storage/LVJJT6Q6/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@misc{hui_alphago_2018,
	title = {{AlphaGo}: {How} it works technically?},
	shorttitle = {{AlphaGo}},
	url = {https://jonathan-hui.medium.com/alphago-how-it-works-technically-26ddcc085319},
	abstract = {How does reinforcement learning join force with deep learning to beat the Go master? Since it sounds implausible, the technology behind it…},
	language = {en},
	urldate = {2024-02-07},
	journal = {Medium},
	author = {Hui, Jonathan},
	month = may,
	year = {2018},
	keywords = {AlphaGo},
	file = {Snapshot:/home/tony/Zotero/storage/HBW88KF8/alphago-how-it-works-technically-26ddcc085319.html:text/html},
}

@book{broy_logische_2019,
	address = {Wiesbaden},
	title = {Logische und {Methodische} {Grundlagen} der {Programm}- und {Systementwicklung}: {Datenstrukturen}, funktionale, sequenzielle und objektorientierte {Programmierung} - {Unter} {Mitarbeit} von {Alexander} {Malkis}},
	isbn = {978-3-658-26301-0 978-3-658-26302-7},
	shorttitle = {Logische und {Methodische} {Grundlagen} der {Programm}- und {Systementwicklung}},
	url = {http://link.springer.com/10.1007/978-3-658-26302-7},
	language = {de},
	urldate = {2024-02-08},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Broy, Manfred},
	year = {2019},
	doi = {10.1007/978-3-658-26302-7},
	file = {Broy - 2019 - Logische und Methodische Grundlagen der Programm- .pdf:/home/tony/Zotero/storage/7SRLEAD6/Broy - 2019 - Logische und Methodische Grundlagen der Programm- .pdf:application/pdf},
}

@book{kramer_computational_2009,
	address = {Berlin, Heidelberg},
	series = {Informatik im {Fokus}},
	title = {Computational {Intelligence}: {Eine} {Einführung}},
	isbn = {978-3-540-79738-8 978-3-540-79739-5},
	shorttitle = {Computational {Intelligence}},
	url = {https://link.springer.com/10.1007/978-3-540-79739-5},
	language = {de},
	urldate = {2024-02-09},
	publisher = {Springer Berlin Heidelberg},
	author = {Kramer, Oliver},
	year = {2009},
	doi = {10.1007/978-3-540-79739-5},
	file = {Kramer - 2009 - Computational Intelligence Eine Einführung.pdf:/home/tony/Zotero/storage/8TZUQANE/Kramer - 2009 - Computational Intelligence Eine Einführung.pdf:application/pdf},
}

@misc{noauthor_using_nodate,
	title = {Using {TensorBoard} to {Observe} {Training} - {Unity} {ML}-{Agents} {Toolkit}},
	url = {https://unity-technologies.github.io/ml-agents/Using-Tensorboard/},
	urldate = {2024-02-09},
	keywords = {Metriken},
	file = {Using TensorBoard to Observe Training - Unity ML-Agents Toolkit:/home/tony/Zotero/storage/PXY29SFN/Using-Tensorboard.html:text/html},
}

@book{ertel_grundkurs_2021,
	address = {Wiesbaden},
	series = {Computational {Intelligence}},
	title = {Grundkurs {Künstliche} {Intelligenz}: {Eine} praxisorientierte {Einführung}},
	isbn = {978-3-658-32074-4 978-3-658-32075-1},
	shorttitle = {Grundkurs {Künstliche} {Intelligenz}},
	url = {https://link.springer.com/10.1007/978-3-658-32075-1},
	language = {de},
	urldate = {2024-02-09},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Ertel, Wolfgang},
	year = {2021},
	doi = {10.1007/978-3-658-32075-1},
	file = {Ertel - 2021 - Grundkurs Künstliche Intelligenz Eine praxisorien.pdf:/home/tony/Zotero/storage/5DDBEV4I/Ertel - 2021 - Grundkurs Künstliche Intelligenz Eine praxisorien.pdf:application/pdf},
}

@article{yuan_novel_2019,
	title = {A novel multi-step reinforcement learning method for solving reward hacking},
	volume = {49},
	issn = {0924-669X, 1573-7497},
	url = {http://link.springer.com/10.1007/s10489-019-01417-4},
	doi = {10.1007/s10489-019-01417-4},
	abstract = {Reinforcement learning with appropriately designed reward signal could be used to solve many sequential learning problems. However, in practice, the reinforcement learning algorithms could be broken in unexpected, counterintuitive ways. One of the failure modes is reward hacking which usually happens when a reward function makes the agent obtain high return in an unexpected way. This unexpected way may subvert the designer’s intentions and lead to accidents during training. In this paper, a new multi-step state-action value algorithm is proposed to solve the problem of reward hacking. Unlike traditional algorithms, the proposed method uses a new return function, which alters the discount of future rewards and no longer stresses the immediate reward as the main influence when selecting the current state action. The performance of the proposed method is evaluated on two games, Mappy and Mountain Car. The empirical results demonstrate that the proposed method can alleviate the negative impact of reward hacking and greatly improve the performance of reinforcement learning algorithm. Moreover, the results illustrate that the proposed method could also be applied to the continuous state space problem successfully.},
	language = {en},
	number = {8},
	urldate = {2024-02-11},
	journal = {Applied Intelligence},
	author = {Yuan, Yinlong and Yu, Zhu Liang and Gu, Zhenghui and Deng, Xiaoyan and Li, Yuanqing},
	month = aug,
	year = {2019},
	pages = {2874--2888},
	file = {Yuan et al. - 2019 - A novel multi-step reinforcement learning method f.pdf:/home/tony/Zotero/storage/E3JYVIKZ/Yuan et al. - 2019 - A novel multi-step reinforcement learning method f.pdf:application/pdf},
}

@misc{schmidt_spiele_gmbh_spielregeln_nodate,
	title = {Spielregeln '{Noch} mal!'},
	shorttitle = {Spielreglen '{Noch} mal!'},
	url = {https://www.schmidtspiele.de/files/Produkte/4/49327%20-%20Noch%20mal!/49327_Noch_Mal_DE.pdf},
	publisher = {Schmidt Spiele GmbH},
	author = {Schmidt Spiele GmbH},
	file = {49327_Noch_Mal_DE.pdf:/home/tony/Zotero/storage/P4DDMST7/49327_Noch_Mal_DE.pdf:application/pdf},
}

@misc{noauthor_alphago_2020,
	title = {{AlphaGo}},
	url = {https://deepmind.google/technologies/alphago/},
	abstract = {Novel AI system mastered the ancient game of Go, defeated a Go world champion, and inspired a new era of AI.},
	language = {en},
	urldate = {2024-03-04},
	journal = {Google DeepMind},
	month = dec,
	year = {2020},
	file = {Snapshot:/home/tony/Zotero/storage/77AGAZ5X/alphago.html:text/html},
}

@misc{noauthor_kunstliche_nodate,
	title = {Künstliche {Neuronale} {Netze} {\textbar} {EF} {Informatik} 2023},
	url = {https://informatik.mygymer.ch/ef2023/07-ki/08-knn.html#kunstliches-neuron},
	abstract = {Informatik Gymnasium Kirchenfeld},
	language = {de-CH},
	urldate = {2024-03-06},
	file = {Snapshot:/home/tony/Zotero/storage/WW8E8BJX/08-knn.html:text/html},
}

@book{lorenz_reinforcement_2020,
	address = {Berlin, Heidelberg},
	title = {Reinforcement {Learning}: {Aktuelle} {Ansätze} verstehen - mit {Beispielen} in {Java} und {Greenfoot}},
	isbn = {978-3-662-61650-5 978-3-662-61651-2},
	shorttitle = {Reinforcement {Learning}},
	url = {http://link.springer.com/10.1007/978-3-662-61651-2},
	language = {de},
	urldate = {2024-03-08},
	publisher = {Springer Berlin Heidelberg},
	author = {Lorenz, Uwe},
	year = {2020},
	doi = {10.1007/978-3-662-61651-2},
	file = {Lorenz - 2020 - Reinforcement Learning Aktuelle Ansätze verstehen.pdf:/home/tony/Zotero/storage/IGE2YC7S/Lorenz - 2020 - Reinforcement Learning Aktuelle Ansätze verstehen.pdf:application/pdf},
}

@article{kaelbling_reinforcement_1996,
	title = {Reinforcement {Learning}: {A} {Survey}},
	volume = {4},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {Reinforcement {Learning}},
	url = {https://www.jair.org/index.php/jair/article/view/10166},
	doi = {10.1613/jair.301},
	abstract = {This paper surveys the field of reinforcement learning from    a computer-science perspective. It is written to be accessible to    researchers familiar with machine learning.  Both the historical basis    of the field and a broad selection of current work are summarized.    Reinforcement learning is the problem faced by an agent that learns    behavior through trial-and-error interactions with a dynamic    environment.  The work described here has a resemblance to work in    psychology, but differs considerably in the details and in the use of    the word ``reinforcement.''  The paper discusses central issues of    reinforcement learning, including trading off exploration and    exploitation, establishing the foundations of the field via Markov    decision theory, learning from delayed reinforcement, constructing    empirical models to accelerate learning, making use of generalization    and hierarchy, and coping with hidden state.  It concludes with a    survey of some implemented systems and an assessment of the practical    utility of current methods for reinforcement learning.},
	language = {en},
	urldate = {2024-03-14},
	journal = {Journal of Artificial Intelligence Research},
	author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
	month = may,
	year = {1996},
	pages = {237--285},
	file = {Full Text PDF:/home/tony/Zotero/storage/9L596ZMR/Kaelbling et al. - 1996 - Reinforcement Learning A Survey.pdf:application/pdf},
}
