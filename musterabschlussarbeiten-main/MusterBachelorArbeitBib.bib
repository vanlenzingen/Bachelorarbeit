
@book{zai_einstieg_2020,
	address = {München},
	title = {Einstieg in {Deep} {Reinforcement} {Learning}: {KI}-{Agenten} mit {Python} und {PyTorch} programmieren},
	isbn = {978-3-446-45900-7},
	shorttitle = {Einstieg in {Deep} {Reinforcement} {Learning}},
	language = {en},
	publisher = {Hanser},
	author = {Zai, Alex and Brown, Brandon},
	year = {2020},
	file = {Zai and Brown - 2020 - Einstieg in Deep Reinforcement Learning KI-Agente.pdf:/home/tony/Zotero/storage/HXZRLXVX/Zai and Brown - 2020 - Einstieg in Deep Reinforcement Learning KI-Agente.pdf:application/pdf},
}

@book{sutton_reinforcement_2014,
	address = {Cambridge, Massachusetts},
	edition = {Nachdruck},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-19398-6},
	shorttitle = {Reinforcement learning},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew},
	year = {2014},
	file = {Sutton and Barto - 2014 - Reinforcement learning an introduction.pdf:/home/tony/Zotero/storage/I3L62CDK/Sutton and Barto - 2014 - Reinforcement learning an introduction.pdf:application/pdf},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the beneﬁts of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	language = {en},
	urldate = {2024-01-26},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:/home/tony/Zotero/storage/LVJJT6Q6/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@misc{hui_alphago_2018,
	title = {{AlphaGo}: {How} it works technically?},
	shorttitle = {{AlphaGo}},
	url = {https://jonathan-hui.medium.com/alphago-how-it-works-technically-26ddcc085319},
	abstract = {How does reinforcement learning join force with deep learning to beat the Go master? Since it sounds implausible, the technology behind it…},
	language = {en},
	urldate = {2024-02-07},
	journal = {Medium},
	author = {Hui, Jonathan},
	month = may,
	year = {2018},
	keywords = {AlphaGo},
	file = {Snapshot:/home/tony/Zotero/storage/HBW88KF8/alphago-how-it-works-technically-26ddcc085319.html:text/html},
}

@book{broy_logische_2019,
	address = {Wiesbaden},
	title = {Logische und {Methodische} {Grundlagen} der {Programm}- und {Systementwicklung}: {Datenstrukturen}, funktionale, sequenzielle und objektorientierte {Programmierung} - {Unter} {Mitarbeit} von {Alexander} {Malkis}},
	isbn = {978-3-658-26301-0 978-3-658-26302-7},
	shorttitle = {Logische und {Methodische} {Grundlagen} der {Programm}- und {Systementwicklung}},
	url = {http://link.springer.com/10.1007/978-3-658-26302-7},
	language = {de},
	urldate = {2024-02-08},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Broy, Manfred},
	year = {2019},
	doi = {10.1007/978-3-658-26302-7},
	file = {Broy - 2019 - Logische und Methodische Grundlagen der Programm- .pdf:/home/tony/Zotero/storage/7SRLEAD6/Broy - 2019 - Logische und Methodische Grundlagen der Programm- .pdf:application/pdf},
}

@book{kramer_computational_2009,
	address = {Berlin, Heidelberg},
	series = {Informatik im {Fokus}},
	title = {Computational {Intelligence}: {Eine} {Einführung}},
	isbn = {978-3-540-79738-8 978-3-540-79739-5},
	shorttitle = {Computational {Intelligence}},
	url = {https://link.springer.com/10.1007/978-3-540-79739-5},
	language = {de},
	urldate = {2024-02-09},
	publisher = {Springer Berlin Heidelberg},
	author = {Kramer, Oliver},
	year = {2009},
	doi = {10.1007/978-3-540-79739-5},
	file = {Kramer - 2009 - Computational Intelligence Eine Einführung.pdf:/home/tony/Zotero/storage/8TZUQANE/Kramer - 2009 - Computational Intelligence Eine Einführung.pdf:application/pdf},
}

@misc{noauthor_using_nodate,
	title = {Using {TensorBoard} to {Observe} {Training} - {Unity} {ML}-{Agents} {Toolkit}},
	url = {https://unity-technologies.github.io/ml-agents/Using-Tensorboard/},
	urldate = {2024-02-09},
	keywords = {Metriken},
	file = {Using TensorBoard to Observe Training - Unity ML-Agents Toolkit:/home/tony/Zotero/storage/PXY29SFN/Using-Tensorboard.html:text/html},
}

@book{ertel_grundkurs_2021,
	address = {Wiesbaden},
	series = {Computational {Intelligence}},
	title = {Grundkurs {Künstliche} {Intelligenz}: {Eine} praxisorientierte {Einführung}},
	isbn = {978-3-658-32074-4 978-3-658-32075-1},
	shorttitle = {Grundkurs {Künstliche} {Intelligenz}},
	url = {https://link.springer.com/10.1007/978-3-658-32075-1},
	language = {de},
	urldate = {2024-02-09},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Ertel, Wolfgang},
	year = {2021},
	doi = {10.1007/978-3-658-32075-1},
	file = {Ertel - 2021 - Grundkurs Künstliche Intelligenz Eine praxisorien.pdf:/home/tony/Zotero/storage/5DDBEV4I/Ertel - 2021 - Grundkurs Künstliche Intelligenz Eine praxisorien.pdf:application/pdf},
}

@article{yuan_novel_2019,
	title = {A novel multi-step reinforcement learning method for solving reward hacking},
	volume = {49},
	issn = {0924-669X, 1573-7497},
	url = {http://link.springer.com/10.1007/s10489-019-01417-4},
	doi = {10.1007/s10489-019-01417-4},
	abstract = {Reinforcement learning with appropriately designed reward signal could be used to solve many sequential learning problems. However, in practice, the reinforcement learning algorithms could be broken in unexpected, counterintuitive ways. One of the failure modes is reward hacking which usually happens when a reward function makes the agent obtain high return in an unexpected way. This unexpected way may subvert the designer’s intentions and lead to accidents during training. In this paper, a new multi-step state-action value algorithm is proposed to solve the problem of reward hacking. Unlike traditional algorithms, the proposed method uses a new return function, which alters the discount of future rewards and no longer stresses the immediate reward as the main influence when selecting the current state action. The performance of the proposed method is evaluated on two games, Mappy and Mountain Car. The empirical results demonstrate that the proposed method can alleviate the negative impact of reward hacking and greatly improve the performance of reinforcement learning algorithm. Moreover, the results illustrate that the proposed method could also be applied to the continuous state space problem successfully.},
	language = {en},
	number = {8},
	urldate = {2024-02-11},
	journal = {Applied Intelligence},
	author = {Yuan, Yinlong and Yu, Zhu Liang and Gu, Zhenghui and Deng, Xiaoyan and Li, Yuanqing},
	month = aug,
	year = {2019},
	pages = {2874--2888},
	file = {Yuan et al. - 2019 - A novel multi-step reinforcement learning method f.pdf:/home/tony/Zotero/storage/E3JYVIKZ/Yuan et al. - 2019 - A novel multi-step reinforcement learning method f.pdf:application/pdf},
}

@misc{schmidt_spiele_gmbh_spielregeln_nodate,
	title = {Spielregeln '{Noch} mal!'},
	shorttitle = {Spielreglen '{Noch} mal!'},
	url = {https://www.schmidtspiele.de/files/Produkte/4/49327%20-%20Noch%20mal!/49327_Noch_Mal_DE.pdf},
	publisher = {Schmidt Spiele GmbH},
	author = {Schmidt Spiele GmbH},
	file = {49327_Noch_Mal_DE.pdf:/home/tony/Zotero/storage/P4DDMST7/49327_Noch_Mal_DE.pdf:application/pdf},
}

@misc{noauthor_alphago_2020,
	title = {{AlphaGo}},
	url = {https://deepmind.google/technologies/alphago/},
	abstract = {Novel AI system mastered the ancient game of Go, defeated a Go world champion, and inspired a new era of AI.},
	language = {en},
	urldate = {2024-03-04},
	journal = {Google DeepMind},
	month = dec,
	year = {2020},
	file = {Snapshot:/home/tony/Zotero/storage/77AGAZ5X/alphago.html:text/html},
}

@misc{noauthor_kunstliche_nodate,
	title = {Künstliche {Neuronale} {Netze} {\textbar} {EF} {Informatik} 2023},
	url = {https://informatik.mygymer.ch/ef2023/07-ki/08-knn.html#kunstliches-neuron},
	abstract = {Informatik Gymnasium Kirchenfeld},
	language = {de-CH},
	urldate = {2024-03-06},
	file = {Snapshot:/home/tony/Zotero/storage/WW8E8BJX/08-knn.html:text/html},
}

@book{lorenz_reinforcement_2020,
	address = {Berlin, Heidelberg},
	title = {Reinforcement {Learning}: {Aktuelle} {Ansätze} verstehen - mit {Beispielen} in {Java} und {Greenfoot}},
	isbn = {978-3-662-61650-5 978-3-662-61651-2},
	shorttitle = {Reinforcement {Learning}},
	url = {http://link.springer.com/10.1007/978-3-662-61651-2},
	language = {de},
	urldate = {2024-03-08},
	publisher = {Springer Berlin Heidelberg},
	author = {Lorenz, Uwe},
	year = {2020},
	doi = {10.1007/978-3-662-61651-2},
	file = {Lorenz - 2020 - Reinforcement Learning Aktuelle Ansätze verstehen.pdf:/home/tony/Zotero/storage/IGE2YC7S/Lorenz - 2020 - Reinforcement Learning Aktuelle Ansätze verstehen.pdf:application/pdf},
}

@misc{zhang_study_2018,
	title = {A {Study} on {Overfitting} in {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1804.06893},
	abstract = {Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen “robustly”: commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.},
	language = {en},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
	month = apr,
	year = {2018},
	note = {arXiv:1804.06893 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zhang et al. - 2018 - A Study on Overfitting in Deep Reinforcement Learn.pdf:/home/tony/Zotero/storage/GZJTFDLH/Zhang et al. - 2018 - A Study on Overfitting in Deep Reinforcement Learn.pdf:application/pdf},
}

@misc{pan_effects_2022,
	title = {The {Effects} of {Reward} {Misspecification}: {Mapping} and {Mitigating} {Misaligned} {Models}},
	shorttitle = {The {Effects} of {Reward} {Misspecification}},
	url = {http://arxiv.org/abs/2201.03544},
	abstract = {Reward hacking—where RL agents exploit gaps in misspeciﬁed reward functions—has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspeciﬁed rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspeciﬁcations, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we ﬁnd instances of phase transitions: capability thresholds at which the agent’s behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.},
	language = {en},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Pan, Alexander and Bhatia, Kush and Steinhardt, Jacob},
	month = feb,
	year = {2022},
	note = {arXiv:2201.03544 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Pan et al. - 2022 - The Effects of Reward Misspecification Mapping an.pdf:/home/tony/Zotero/storage/WUXVYDS6/Pan et al. - 2022 - The Effects of Reward Misspecification Mapping an.pdf:application/pdf},
}

@misc{hare_dealing_2019,
	title = {Dealing with {Sparse} {Rewards} in {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1910.09281},
	abstract = {Successfully navigating a complex environment to obtain a desired outcome is a difficult task, that up to recently was believed to be capable only by humans. This perception has been broken down over time, especially with the introduction of deep reinforcement learning, which has greatly increased the difficulty of tasks that can be automated. However, for traditional reinforcement learning agents this requires an environment to be able to provide frequent extrinsic rewards, which are not known or accessible for many real-world environments. This project aims to explore and contrast existing reinforcement learning solutions that circumnavigate the difficulties of an environment that provide sparse rewards. Different reinforcement solutions will be implemented over a several video game environments with varying difficulty and varying frequency of rewards, as to properly investigate the applicability of these solutions. This project introduces a novel reinforcement learning solution by combining aspects of two existing state of the art sparse reward solutions, curiosity driven exploration and unsupervised auxiliary tasks.},
	language = {en},
	urldate = {2024-03-18},
	publisher = {arXiv},
	author = {Hare, Joshua},
	month = nov,
	year = {2019},
	note = {arXiv:1910.09281 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Hare - 2019 - Dealing with Sparse Rewards in Reinforcement Learn.pdf:/home/tony/Zotero/storage/EJYWRWXA/Hare - 2019 - Dealing with Sparse Rewards in Reinforcement Learn.pdf:application/pdf},
}
